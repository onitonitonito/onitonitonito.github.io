---
layout: post
title: ìœ„ì¡°ë²”ê³¼ ê²½ì°°ê´€ - ë¨¸ì‹ ëŸ¬ë‹ GAN Model êµ¬í˜„ğŸš“
comments: true
category: [ML]
tag: [GAN, MNIST]
excerpt_separator: <!-- more -->
---
<img alt="ì¡ì•˜ë‹¤ìš”ë†ˆ!" width="130" align="left" style="padding: 0px 10px 0px 0px;" src="/images/post_img/20180917-0logo.png" >

GAN(Generative Adversarial Nets) ëª¨ë¸ì€ ëŒ€ë¦½í•˜ëŠ” ë‘ê°œì˜ ì‹ ê²½ë§ì„ ì„œë¡œ ê²½ìŸì‹œì¼œ ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

_"Adversarial"ì´ë€ ë‹¨ì–´ì˜ ì‚¬ì „ì  ì˜ë¯¸ë¥¼ ë³´ë©´ ëŒ€ë¦½í•˜ëŠ”, ì ëŒ€í•˜ëŠ” ë€ ëœ»ì„ ê°–ìŠµë‹ˆë‹¤. ëŒ€ë¦½í•˜ë ¤ë©´ ì–´ì°Œ ë˜ì—ˆë“  ìƒëŒ€ê°€ ìˆì–´ì•¼í•˜ë‹ˆ GANì€ í¬ê²Œ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆë‹¤ëŠ” ê²ƒì„ ë¨¼ì € ì§ê´€ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤._
<!-- more -->

<br><br>
## By Tim O'Shea, O'Shea Research.

> Some of the generative work done in the past year or two using generative adversarial networks (GANs) has been pretty exciting and demonstrated some very impressive results.  The general idea is that you train two models, one (G) to generate some sort of output example given random noise as input, and one (A) to discern generated model examples from real examples.  Then, by training A to be an effective discriminator, we can stack G and A to form our GAN, freeze the weights in the adversarial part of the network, and train the generative network weights to push random noisy inputs towards the â€œrealâ€ example class output of the adversarial half.


| <img src="/images/post_img/20180917-gan-illust01.png" width="550"> |
|:----------------------------------------------|
| **Fig.00** - GAN ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ê°œë…ë„ |
|[ì¶œì²˜]: [MNIST Generative Adversarial Model in Keras](https://goo.gl/JL2KNs)|



<br><br>
# 1.0 ì ì  ì§€ëŠ¥í™” ë˜ê°€ëŠ” ìœ„ì¡°ì§€íë²”

GAN ëª¨ë¸ì„ ì œì•ˆí•œ ì´ì•ˆ êµ¿í ë¡œìš°(Ian Goodfellow)ê°€ ë…¼ë¬¸ì— ì œì‹œí•œ ë¹„ìœ ì— ì˜í•˜ë©´, ìœ„ì¡°ì§€íë²”(ìƒì„±ì ì‹ ê²½ë§)ê³¼ ê²½ì°°ê´€(êµ¬ë¶„ì ì‹ ê²½ë§)ì„ ìƒí˜¸ ëŒ€ë¦½ì‹œì¼œ, ê²½ì°°ì€ ê°ë³„í•˜ë ¤ê³  ë…¸ë ¥í•˜ê³ , ìœ„ì¡°ì§€íë²”ì€ ìœ„ì¡°ë°©ë²•ì„ ê³ ë„í™” í•˜ëŠ” ê´€ê³„ë¥¼ í†µí•´ì„œ ì§„ì§œì™€ êµ¬ë¶„í•˜ê¸° ì–´ë ¤ìš´ ê³ ë„ì˜ ìœ„ì¡°ì§€íë¥¼ ë§Œë“¤ìˆ˜ ìˆë„ë¡ í•™ìŠµ í•˜ê²Œ í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.

| <img src="/images/post_img/20180917-gan-illust03.png" width="600"> |
|:----------------------------------------------|
| **Fig.00** - GAN ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ” ê°œë…ë„ |
|[ì¶œì²˜]: [MNIST Generative Adversarial Model in Keras](https://goo.gl/JL2KNs)|

<br><br>
# 2.0 MNIST ë¥¼ ì´ìš©í•œ GANëª¨ë¸ ì‘ì„±
* ì²˜ìŒì—ëŠ” ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ë…¸ì´ì¦ˆë¥¼ ë°œìƒ ì‹œí‚¤ì§€ë§Œ
* í•™ìŠµì˜ ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ì„œ, ì ì  ìœ„ì¡°ëª¨ë°©ì´ êµ¬ì²´í™” ë˜ì–´ ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

>
| <img src="/images/post_img/20180917-gan-001.png" width="550"> |
|:-----------------------------------------|
| **Fig.01** - ì²˜ìŒ ì œë„ˆë ˆì´í„°ëŠ” ì•„ë¬´ê±°ë‚˜, ë§‰ë˜ì ¸ë³´ëŠ” ì‹.. ê·¸ëƒ¥, ë…¸ì´ì¦ˆ ì¼ ë¿! |
>
| <img src="/images/post_img/20180917-gan-010.png" width="550"> |
|:-----------------------------------------|
| **Fig.02** - ì´í„°ë ˆì´ì…˜ì´ ë°˜ë³µë  ìˆ˜ë¡ ëª¨ë°©ì€ ì ì  ì •êµí•´ì§„ë‹¤. |
>
| <img src="/images/post_img/20180917-gan-030.png" width="550"> |
|:-----------------------------------------|
| **Fig.03** - ìƒë‹¹íˆ ì •êµí•´ ì¡Œë‹¤.. ì´ì œ ê²¨ìš° 300ë²ˆ ë°˜ë³µ í•™ìŠµ |
>
| <img src="/images/post_img/20180917-gan-200.png" width="550"> |
|:-----------------------------------------|
| **Fig.04** - ì´ë¯¸ 1,000íšŒë¥¼ ë„˜ì—ˆì„ë•Œ ì •êµí™” ê³¼ì •ì€ ëë‚¬ë‹¤. |




<br><br>
# 3.0 ì›ë³¸ EXAMPLE ê³¼ ìœ„ì¡° ê²°ê³¼ë¬¼ ë¹„êµ
* ì‚¬ì‹¤, ì—¬ê¸°ì„œ ë³´ì—¬ì£¼ëŠ” ì†ê¸€ì”¨ëŠ” __MNIST__ ì— ì—†ëŠ” **ì „í˜€ ìƒˆë¡œìš´** ëª¨ë°©ëœ ì†ê¸€ì”¨ ë°ì´í„°ë‹¤.
* __ì„¸ìƒì— ì—†ëŠ” ì°½ì¡° ëœ ë°ì´í„°!__

| <img src="/images/post_img/20180917-gan-letter01.png" width="250">ã€€ ã€€| <img src="/images/post_img/20180917-gan-letter02.png" width="250"> |
|:----------------:|:----------------:|
|**01.ì›ë³¸ ì´ë¯¸ì§€** | **02.ìœ„ì¡° ì´ë¯¸ì§€**|



<br><br>
# 2.0 ì „ì²´ ëª¨ë°©ì´ ì§„í–‰ë˜ëŠ” ê³¼ì •
* ê°, ì´ë¯¸ì§€ë‹¹ 100íšŒì˜ í•™ìŠµì„ ê±°ì³ì„œ êµì •ì´ ì´ë£¨ì–´ ì¡Œë‹¤
* ì•½ 1,000íšŒë¥¼ ë„˜ì—ˆì„ë•Œ ì´ë¯¸ ì •êµí™” ê³¼ì •ì€ ëë‚œë“¯~ (í’ˆì§ˆì´ ë¹„ìŠ·)


| <img src="/images/post_img/20180917-gan-000.png" width="550"> |
|:-----------------------------------------|
| **Fig.04** - ê¸°ê³„í•™ìŠµ, í”¼ë“œë°±ì„ ë°›ì•„ ì†ê¸€ì”¨ë¥¼ ëª¨ë°©í•˜ëŠ” ê³¼ì • |


<br><br><br>
# 3.0 ì°¸ê³ ìë£Œ
1. [MNIST Generative Adversarial Model in Keras](https://goo.gl/JL2KNs)
1. [Jaejun Yoo's Playground](https://goo.gl/ZvSvtm): ì´ˆì§œ ëŒ€í•™ì›ìƒì´ ì´í•´í•˜ëŠ” Generative Adversarial Nets
1. [ê³¨ë¹ˆí•´ì»¤ì˜ 3ë¶„ ë”¥ëŸ¬ë‹](https://goo.gl/rZF2Rx) - í…ì„œí”Œë¡œìš° ë§› .... ( í•œë¹›ë¯¸ë””ì–´ / ê¹€ì§„ì¤‘ )


<br><br>
# CODE PART.01 - ë³€ìˆ˜ì •ì˜

```python
import os
import sys
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# 'ë£¨íŠ¸'ì™€ 'ì‘ì—…'ë””ë ‰í† ë¦¬ ì„¤ì • - for ìŠ¤í¬ë¦½íŠ¸ëŸ°
DIRS = os.path.dirname(__file__).partition("deep_MLDL")
ROOT = DIRS[0] + DIRS[1]
sys.path.append(ROOT)

from os.path import dirname, join
WORK_DIR = join(ROOT,'_static','MNIST_data','')


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(WORK_DIR, one_hot=True)

# Hyper- Parameter
learning_rate = 2e-4
training_epoches = 100
batch_size = 100

n_hidden = 256
n_input = 28*28             # 784 pix. for 1 letter
n_noise = 128

# placeholder
X = tf.placeholder(tf.float32, [None, n_input])
Z = tf.placeholder(tf.float32, [None, n_noise])

G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))
G_b1 = tf.Variable(tf.zeros([n_hidden]))

G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))
G_b2 = tf.Variable(tf.zeros([n_input]))

D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))
D_b1 = tf.Variable(tf.zeros([n_hidden]))

D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))
D_b2 = tf.Variable(tf.zeros([1]))
```


<br><br>
# CODE PART.0 - í•¨ìˆ˜ì •ì˜

```python
def generator(noise_z):
    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)
    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)
    return output

def discriminator(inputs):
    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)
    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)
    return output

def get_noise(batch_size, n_noise):
    return np.random.normal(size=(batch_size, n_noise))
```


<br><br>
# CODE PART.03 - ë³€ìˆ˜ì •ì˜

```python
G = generator(Z)
D_gene = discriminator(G)
D_real = discriminator(X)

cost_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_gene))
cost_G = tf.reduce_mean(tf.log(D_gene))

D_var_list = [D_W1, D_b1, D_W2, D_b2]
G_var_list = [G_W1, G_b1, G_W2, G_b2]

train_D = tf.train.AdamOptimizer(learning_rate).minimize(-cost_D, var_list=D_var_list)
train_G = tf.train.AdamOptimizer(learning_rate).minimize(-cost_G, var_list=G_var_list)
```





<br><br>
# CODE PART.04 - ê·¸ë˜í”„ ê·¸ë¦¬ê¸°

```python
# Draw Graph - Neural Networ training
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

total_batch = int(mnist.train.num_examples / batch_size)
loss_val_D, loss_val_G = 0, 0

for epoch in range(training_epoches):
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        noise = get_noise(batch_size=batch_size, n_noise=n_noise)

    # calculate train(optimize), cost function as pair
    _, loss_val_D = sess.run([train_D, cost_D], feed_dict={X:batch_xs, Z:noise})
    _, loss_val_G = sess.run([train_G, cost_G], feed_dict={Z:noise})

    if epoch%10 == 0:
        print("Epoch:%3s ___ Cost_D: %.9f"% (epoch, loss_val_D))
        print("          ___ Cost_G: %.9f\n"% (loss_val_G))

    # Check generated image
    if epoch == 0 or (epoch+1)%10 == 0:
        sample_size = 10
        noise = get_noise(sample_size, n_noise)
        samples = sess.run(G, feed_dict={Z:noise})
        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))

        for i in range(sample_size):
            ax[i].set_axis_off()
            ax[i].imshow(np.reshape(samples[i], (28, 28)))

        plt.savefig(WORK_DIR + '{}.png'.format(
            str(epoch+1).zfill(3)),
            bbox_inches = 'tight')
        plt.close(fig)

print('...optimizing finished ...')
```
